{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d728bfc-2c40-4bec-9f2c-5fbf6dc0e3e2",
   "metadata": {},
   "source": [
    "# Corpus from OpenRefine\n",
    "\n",
    "The notebook download and merge all the sources tagged with #corpusVC in OpenRefine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279208ba-5d5b-4025-a099-5626f71f3c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f62ba8-98dd-47ec-9fd4-31d534b6b9df",
   "metadata": {},
   "source": [
    "## Step 1. Create folder for the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05902bf3-65c3-4928-a562-4e4c96d34964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date in \"YYYY-MM-DD\" format\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define the folder name based on the current date\n",
    "folder_name = f\"data_OR_{current_date}\"\n",
    "\n",
    "# Get the script's directory (where this code is executed)\n",
    "script_directory = os.getcwd()\n",
    "\n",
    "# Create the full path for the new folder in the script's directory\n",
    "folder_path = os.path.join(script_directory, folder_name)\n",
    "\n",
    "# Check if the folder already exists and create it if not\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder '{folder_name}' created in '{script_directory}'\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists in '{script_directory}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614101f-a068-4468-b91c-9b33189fc8ed",
   "metadata": {},
   "source": [
    "## Step 2. Download datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd23aa4-3bce-4803-84ac-06833919742c",
   "metadata": {},
   "source": [
    "### Step 2.1 LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d294bcaf-bd80-4293-85ec-7d5e0f43a4f0",
   "metadata": {},
   "source": [
    "#### Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69189ed-efe9-4116-bb1b-4a50c5629a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the file to download\n",
    "file_url = 'https://jdp.visualcontagions.net/data/loc.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2e673-69f2-49c8-b8a9-799d92a81150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the file name from the URL\n",
    "file_name = os.path.basename(file_url)\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "# Stream the file download with a progress bar\n",
    "response = requests.get(file_url, stream=True)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Get the total file size (in bytes)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "    # Create a progress bar using tqdm\n",
    "    progress_bar = tqdm(total=total_size, unit='B', unit_scale=True)\n",
    "\n",
    "    # Open a file for writing\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            # Write the downloaded data to the file\n",
    "            file.write(data)\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(len(data))\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    print(f\"File '{file_name}' downloaded and saved in '{folder_path}'\")\n",
    "else:\n",
    "    # Handle errors\n",
    "    print(f\"Download failed with status code {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98895c9-2fce-4cf4-8177-a980e697fd56",
   "metadata": {},
   "source": [
    "#### Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a9f0d-5f9a-4510-89e9-df466d4872c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_source_file = \"/Users/carboni/Documents/UNIGE/pynotebook/OpenRefine_fusion_Local/LOC/LOC.csv\"\n",
    "shutil.move(loc_source_file, folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25072a4b-c04c-4986-8312-7420d8ce7f20",
   "metadata": {},
   "source": [
    "### Step 2.2. Everything Else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c0c382-9ae8-4a9d-99ce-030aa322bc7e",
   "metadata": {},
   "source": [
    "**List of the projects from OpenRefine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d8815-37e1-46f9-81a1-3d3d5e443661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the API endpoint URL\n",
    "api_url = 'http://129.194.213.75/command/core/get-all-project-metadata'\n",
    "\n",
    "# Disable SSL certificate verification (use with caution)\n",
    "verify_ssl = False\n",
    "\n",
    "# Make the GET request to the API\n",
    "response = requests.get(api_url, verify=verify_ssl)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    api_response = response.json()\n",
    "else:\n",
    "    print(f\"API request failed with status code: {response.status_code}\")\n",
    "    print(response.text)\n",
    "\n",
    "projects_data = api_response.get('projects', {})\n",
    "\n",
    "# Initialize an empty dictionary for projects\n",
    "projects_dict = {}\n",
    "\n",
    "# Iterate through the projects\n",
    "for project_id, project_info in projects_data.items():\n",
    "    project_tags = project_info.get('tags', [])\n",
    "    if 'corpusVC' in project_tags:\n",
    "        project_name = project_info.get('name', '')\n",
    "        projects_dict[project_id] = project_name\n",
    "\n",
    "print(projects_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c50f7d-1c5d-4e6a-aa1e-b4b7bd9ad879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API endpoint and parameters\n",
    "url = 'http://129.194.213.75/command/core/export-rows'\n",
    "format = 'csv'\n",
    "engine = '{\"facets\":[],\"mode\":\"row-based\"}'\n",
    "\n",
    "# Dictionary of project IDs to project names\n",
    "\n",
    "# the folder is already specified in step 1\n",
    "#folder_path = 'project_refine'\n",
    "\n",
    "# Iterate over the projects and their names\n",
    "for project_id, project_name in projects_dict.items():\n",
    "    # Set the project parameter\n",
    "    params = {\n",
    "        'project': project_id,\n",
    "        'format': format,\n",
    "        'engine': engine\n",
    "    }\n",
    "\n",
    "    # Specify the file name for each project\n",
    "    file_name = f'{project_name}.csv'\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Send the POST request\n",
    "    response = requests.post(url, params=params)\n",
    "\n",
    "    # Check the response status and save the CSV data to a file\n",
    "    if response.status_code == 200:\n",
    "        # The request was successful, save the CSV data to the file\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"CSV data for '{project_name}' (ID: {project_id}) saved as '{file_name}' in '{folder_path}'\")\n",
    "    else:\n",
    "        # Handle errors\n",
    "        print(f\"Request for '{project_name}' (ID: {project_id}) failed with status code {response.status_code}\")\n",
    "        print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc522f1-ead1-415d-aec5-5dce3f4809f9",
   "metadata": {},
   "source": [
    "## Step 3. Merge and Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740dc701-0bf6-4c37-b0c5-2c5d310a45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_col = ['Media URL','City','Country','wkt', 'normalized_date', 'Title', 'Journal Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93346b-6cd4-4458-a76f-fa6cd09e55b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all CSV files in the folder\n",
    "all_files = glob.glob(folder_path + \"/*.csv\")\n",
    "\n",
    "# Iterate over the files and delete those with 'CorpusCombined' in the filename\n",
    "for filename in all_files:\n",
    "    if 'CorpusCombined' in filename:\n",
    "        os.remove(filename)\n",
    "        print(f\"Deleted '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca304b-fb4e-4a58-b490-b5c754929c75",
   "metadata": {},
   "source": [
    "### Step 3.1 Create 2 datasets, one with LOC and one without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e473f-0595-4496-8ba3-9415d64b5024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with LOC\n",
    "\n",
    "all_files = glob.glob(folder_path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0, delimiter=',', low_memory=False)\n",
    "    df = df[keep_col]\n",
    "    df['source'] = filename.split(\"/\")[-1]\n",
    "    li.append(df)\n",
    "\n",
    "df_merged = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770ab3b-1580-41e9-958e-24e1eef33b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without LOC + sources\n",
    "\n",
    "all_files = glob.glob(folder_path + \"/*.csv\")\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    # Skip processing 'loc.csv'\n",
    "    if 'loc.csv' in filename:\n",
    "        continue    \n",
    "    df = pd.read_csv(filename, index_col=None, header=0, delimiter=',', low_memory=False)    \n",
    "    df = df[keep_col]   \n",
    "    df['source'] = filename.split(\"/\")[-1]\n",
    "    li.append(df)\n",
    "    \n",
    "df_merged_sans_loc = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4d740-b839-47f6-8c41-0445142ae2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_sans_loc.to_csv('combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360afbb2-c285-487d-8a53-cfac7fa68ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_light = df_merged_sans_loc.drop_duplicates(subset='Title', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a5331-e721-4548-894c-d5c4ee0f1c09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_light.to_csv('combined_light.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5d332-0cf3-4c1b-b2d8-5d12911b8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_sans_loc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1930b-2bfe-4689-9c88-1ce48f1c0cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34896b02-f8ae-48f6-aa7e-ff5abdfb418b",
   "metadata": {},
   "source": [
    "### Step 3.2 Save the content of the dataset without LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095fbfb0-d589-482e-936b-5f39e3a6a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_name = f\"CorpusCombined_{current_date}.csv\"\n",
    "merged_path = os.path.join(folder_path, merged_name)\n",
    "df_merged_sans_loc.to_csv(merged_path, index=False)\n",
    "#df_merged.to_csv(merged_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860d3da-96e5-4c5e-9445-09d0e489d528",
   "metadata": {},
   "source": [
    "### Number of Journal and issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342ead9-b237-48d6-be36-97e48b6cd2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"Title\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e988dcf-2b2e-4cd8-bd4a-026b97659cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"Media URL\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9b0936-5be3-48cd-90f6-ca814d0d0110",
   "metadata": {},
   "source": [
    "### Cities and Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e03cb-fc3b-4943-a214-d4109d5a98df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"City\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed1cb2-4eb3-4462-9722-6bfddcbf9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"Country\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1c15c-ffef-45f1-89f0-e2ffe45f1e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072c37d-548d-45b5-8983-34193719129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = df_merged['Country'].value_counts().reset_index()\n",
    "country_counts.columns = ['Country', 'Count']\n",
    "\n",
    "# Create the Icicle Chart\n",
    "fig = px.icicle(country_counts, path=['Country'], values='Count', \n",
    "                title='Country Frequency in DataFrame')\n",
    "\n",
    "# Show the chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b7075-8404-4d45-b026-0dc9357d09a3",
   "metadata": {},
   "source": [
    "### Journal type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ec15b-4a3c-4698-8e03-5d6a2d0302d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"Journal Type\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745cedba-3f3f-431f-b88b-28eb2d6647c7",
   "metadata": {},
   "source": [
    "### Earliest and latest date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581157c-53d6-4af1-8f6d-89cdbc1fe276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['normalized_date'] =  pd.to_datetime(df_merged['normalized_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0d7664-1e18-47d4-a5ea-289009b57470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['normalized_date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86297431-73a1-4c5a-b882-d051ea59287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['normalized_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f096957d-a4ab-49b6-8d11-e1c10dcb71cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d154df-6a9f-48ae-8cd5-e069d905ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('/Users/carboni/Downloads/merged.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
